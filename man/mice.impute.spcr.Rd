% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mice.impute.spcr.R
\name{mice.impute.spcr}
\alias{mice.impute.spcr}
\alias{spcr}
\title{Imputation by supervised principal component regression}
\usage{
mice.impute.spcr(
  y,
  ry,
  x,
  wy = NULL,
  theta = seq(0.1, 0.9, by = 0.1),
  npcs = 1,
  nfolds = 10,
  ...
)
}
\arguments{
\item{y}{Vector to be imputed}

\item{ry}{Logical vector of length \code{length(y)} indicating the
the subset \code{y[ry]} of elements in \code{y} to which the imputation
model is fitted. The \code{ry} generally distinguishes the observed
(\code{TRUE}) and missing values (\code{FALSE}) in \code{y}.}

\item{x}{Numeric design matrix with \code{length(y)} rows with predictors for
\code{y}. Matrix \code{x} may have no missing values.}

\item{wy}{Logical vector of length \code{length(y)}. A \code{TRUE} value
indicates locations in \code{y} for which imputations are created.}

\item{theta}{Vector of possible values for the association-threshold on
which cross-validation will be performed.}

\item{npcs}{The number of principal components to keep or the automatic method
to use for decision.}

\item{nfolds}{The number of folds for the cross-validation of the association-threshold.}

\item{...}{Other named arguments.}
}
\value{
Vector with imputed data, the same type as \code{y}, and of length
\code{sum(wy)}
}
\description{
Imputes univariate missing data using supervised principal component regression.
}
\details{
Imputation of \code{y} by supervised principal component regression (Bair, 2006).
The method consists of the following steps:
\enumerate{
\item All of the potential predictors returning an R-squared larger than a 
threshold \eqn{\theta} are selected as an active set of predictors.
\item For a given \code{y} variable under imputation, draw a bootstrap version y*
with replacement from the observed cases \code{y[ry]}, and stores in x* the
corresponding values from \code{x[ry, ]}.
\item Regress \code{y*} on every potential predictor (simple linear 
regression), and store their R-squared.
\item Fit a PC regression with \code{y*} as the outcome, the predictors in 
\code{x*} whose simple regression R-square exceeds a defined threshold,
and \code{npcs} components.
\item Calculate the estimated residual standard error \code{sigma} based on the residuals
obtained from the PC regression and \code{npcs - 1} degrees of freedom.
\item Obtain predicted values for \code{y} based on the fitted PC regression
and the new data \code{x[wy, ]}
\item Obtain imputations by adding noise scaled by \code{sigma} to these
predictions.
}

K-fold cross-validation is used to select a threshold value among a user-defined
grid of values.
For every provided threshold in the range [0, 1], all predictors with an R-square larger
than the threshold form an active set of predictors.
Then, \code{npcs} PCs are extracted from each active set and used to predict 
the dependent variable.
The active set giving best K-fold validation MSE is kept.

This function allows the specification of a custom value for `npcs`.
In a dataset where few predictors are associated with the variables under imputation,
the requested number of PCs may be larger than the number of predictors
selected for a given cross-validated threshold.
If this is the case, the maximum number of PCs supported is used.
This means that the predictors selected based on the cross-validation threshold
are projected on a new space where they are independent, but no dimensionality
reduction is performed.
Similarly, it may happen that for a given threshold value, fewer predictors 
are kept than the number of PCs requested by the user.
In this case, the maximum number of npcs supported is used in the cross-validation procedure.

The user can specify a \code{predictorMatrix} in the \code{mice} call
to define which predictors are provided to this univariate imputation method.
Therefore, users may force the exclusion of a predictor from a given
imputation model by speficing a \code{0} entry.
However, a non-zero entry does not guarantee the variable will be used,
as this decision is ultimately made based on the k-fold cross-validation
procedure.
}
\references{
Bair, E., Hastie, T., Paul, D., & Tibshirani, R. (2006). Prediction by
supervised principal components. Journal of the American Statistical
Association, 101(473), 119-137.
}
\seealso{
Other univariate imputation functions: 
\code{\link{mice.impute.cart}()},
\code{\link{mice.impute.lasso.logreg}()},
\code{\link{mice.impute.lasso.norm}()},
\code{\link{mice.impute.lasso.select.logreg}()},
\code{\link{mice.impute.lasso.select.norm}()},
\code{\link{mice.impute.lda}()},
\code{\link{mice.impute.logreg.boot}()},
\code{\link{mice.impute.logreg}()},
\code{\link{mice.impute.mean}()},
\code{\link{mice.impute.midastouch}()},
\code{\link{mice.impute.mnar.logreg}()},
\code{\link{mice.impute.mpmm}()},
\code{\link{mice.impute.norm.boot}()},
\code{\link{mice.impute.norm.nob}()},
\code{\link{mice.impute.norm.predict}()},
\code{\link{mice.impute.norm}()},
\code{\link{mice.impute.pcovr}()},
\code{\link{mice.impute.pcr}()},
\code{\link{mice.impute.pls}()},
\code{\link{mice.impute.pmm}()},
\code{\link{mice.impute.polr}()},
\code{\link{mice.impute.polyreg}()},
\code{\link{mice.impute.quadratic}()},
\code{\link{mice.impute.rf}()},
\code{\link{mice.impute.ri}()}
}
\author{
Edoardo Costantini, 2022
}
\concept{univariate imputation functions}
\keyword{imputation}
